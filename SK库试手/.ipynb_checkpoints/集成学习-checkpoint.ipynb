{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "bagging=BaggingClassifier(KNeighborsClassifier())\n",
    "\n",
    "#参数\n",
    "base_estimator=None #基学习器\n",
    "n_estimators=10 #数量\n",
    "max_samples=1.0 #每一个基学习器的抽样数量或比例0.5\n",
    "max_features=1.0 #特征数量或比例\n",
    "bootstrap=True #True 则有放回，False无放回\n",
    "bootstrap_features=False #\n",
    "oob_score=False #包外估计泛化精度\n",
    "\n",
    "#方法\n",
    "bagging.classes_ #类标签\n",
    "bagging.oob_score_ #包外估计得分\n",
    "decision_function（X）#基本分类器的决策函数的平均值，仅分类\n",
    "predict（X）#预测X的类\n",
    "predict_proba（X）#预测X的类概率，仅分类\n",
    "score（X，y [，sample_weight]）#返回给定测试数据和标签的平均精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#随机森林\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "#参数\n",
    "n_estimators=10\n",
    "criterion=’gini’ #“entropy”  回归：“mae”，default=”mse”\n",
    "max_depth=None\n",
    "min_samples_split=2 #中间节点拆分条件\n",
    "min_samples_leaf=1 #叶节点最小\n",
    "min_weight_fraction_leaf=0.0\n",
    "max_features=’auto’ #寻找最佳分割时要考虑的功能数量/比例\n",
    "bootstrap=True\n",
    "oob_score=False\n",
    "class_weight = None #类别权重字典\n",
    "\n",
    "#方法\n",
    "classes_\n",
    "n_classes_\n",
    "feature_importances_\n",
    "apply（X）#将森林中的树木施用于X，返回叶指数。\n",
    "decision_path（X）#返回森林中的决策路径\n",
    "predict_proba（X）#预测X的类概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#极端随机树：1）ET中每棵树采用所有训练样本，即每棵树的样本集相同。2）RF在特征子集中选择最优分叉特征，而ET直接随机选择分叉特征。\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "#同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#全部随机树嵌入：用叶索引来编码样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "#分类\n",
    "#参数\n",
    "base_estimator=None #常用决策树和神经网络，默认使用CART分类树\n",
    "n_estimators=50\n",
    "learning_rate=1.0\n",
    "algorithm=’SAMME.R’#‘SAMME.R’升压算法，则弱学习器必须支持概率。'SAMME'，离散增强算法\n",
    "\n",
    "#方法\n",
    "staged_predict（X）#返回X的分期预测\n",
    "staged_score（X，y [，sample_weight]）#返回X，y的分期分数。\n",
    "\n",
    "#回归 AdaBoost.R2回归算法\n",
    "#参数\n",
    "base_estimator = None #default=DecisionTreeRegressor\n",
    "n_estimators = 50\n",
    "learning_rate = 1.0\n",
    "loss ='linear' #‘square', ‘exponential’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#梯度提升树\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#分类\n",
    "#参数\n",
    "loss=’deviance’#损失函数'deviance'均方误差，类似逻辑回归，'exponential'指数损失函数，和adaboost用的一样，选指数损失函数只能用二分类问题\n",
    "learning_rate=0.1 \n",
    "n_estimators=100 #梯度提升对于过度拟合是相当稳健的，所以大量的通常会导致更好的性能\n",
    "subsample=1.0 #是否抽样或使用全样本\n",
    "criterion=’friedman_mse’#衡量划分质量，残差。均方误差为“mse”，平均绝对误差为“mae”。“friedman_mse”的默认值通常是最好的\n",
    "min_samples_split=2\n",
    "min_samples_leaf=1\n",
    "max_depth=3 #个体回归估计量的最大深度\n",
    "max_features=None\n",
    "\n",
    "#方法\n",
    "feature_importances_\n",
    "oob_improvement_ #oob_improvement_[0]是第一阶段包外残差的提升\n",
    "train_score_ #train_score_[i]是第i次迭代的残差\n",
    "apply（X）\t将树组合应用于X，返回叶指数。\n",
    "decision_function（X）\t计算决策函数X。\n",
    "predict（X）\t预测X的类\n",
    "predict_proba（X）\t预测X的类概率。\n",
    "score（X，y [，sample_weight]）\t返回给定测试数据和标签的平均精度。\n",
    "staged_predict（X）\t在X的每个阶段预测类\n",
    "staged_predict_proba（X）\t预测X的每个阶段的类概率\n",
    "\n",
    "#回归\n",
    "#参数\n",
    "loss='ls'，'lad'，'huber'，'quantile'#'ls'是指最小二乘回归。'lad'（最小绝对偏差）是基于输入变量的顺序信息的强大的损失函数。'huber'是两者的结合。“quantile”允许分位数回归\n",
    "\n",
    "#方法\n",
    "#同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#部分依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#投票\n",
    "from  sklearn.ensemble import VotingClassifier\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard'，weights=[2,5,1]) #权重\n",
    "scores = cross_val_score(eclf, X, y, cv=5, scoring='accuracy')\n",
    "eclf = eclf.fit(X,y)\n",
    "#voting='soft'软投票概率支持\n",
    "\n",
    "#with GridSearch\n",
    "#调参选参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
